#!/bin/bash -eu

#
# common.sh.in  common code shared across experiment scripts
# 02-Mar-2017  gamvrosi@cs.cmu.edu
#

#
# global variables we set/use:
#  $all_nodes - list of all nodes (sep: whitespace)
#  $bb_log_size - BBOS max per-core log size in bytes
#  $bbos_buddies - number of nodes for bbos
#  $bbos_nodes - list of nodes for bbos
#  $cores - total cores across all nodes
#  $dfsu_prefix - deltafs umbrella prefix directory
#  $do_querying - whether we will perform particle queries
#  $ip_subnet - the ip subnet we want to use
#  $jobdir - per-job shared output directory
#  $logfile - job's logfile we append to (shared among all exps)
#  $exp_logfile - exp's own logfile
#  $nodes - number of nodes for vpic
#  $vpic_nodes - list of nodes for vpic
#

# environment variables we set/use:
#  $JOBDIRHOME - where to put job dirs (default: $HOME/jobs)
#                example: /lustre/ttscratch1/users/$USER
#  $EXTRA_MPIOPTS - additional options that need to be
#                       passed to mpirun
#

#
# environment variables we use as input:
#  $HOME - your home directory
#  $MOAB_JOBNAME - jobname (cray)
#  $PBS_JOBID - job id (cray)
#  $PBS_NODEFILE - file with list of all nodes (cray)
#

#
# files we create:
#  $jobdir/hosts.txt - list of hosts
#  $jobdir/bbos.hosts - host file only of bbos hosts
#  $jobdir/vpic.hosts - host file only of vpic hosts
#

# TODO:
# - Convert node lists to ranges on CRAY

#
# prefix directory comes from cmake's ${CMAKE_INSTALL_PREFIX} variable
#
dfsu_prefix=@CMAKE_INSTALL_PREFIX@

### ensure definition of a set of global vars ###

#
# number of epochs for VPIC runs
#
vpic_epochs=8

#
# job-wise log - shared among all exp runs
#
logfile=""

#
# exp-wise log - one per exp run
#
exp_logfile=""

#
# num of bbos nodes to use
#
bbos_buddies=0

#
# message: echo message to stdout, cc it to a default job-wise log file, and
# then cc it again to a specific exp-wise log file.
# note that if either $logfile or $exp_logfile is empty, tee will
# just print the message without writing to files
# uses: $logfile, $exp_logfile
#
message () { echo "$@" | tee -a $logfile | tee -a $exp_logfile; }

#
# die: emit a mesage and exit 1
#
die () { message "Error $@"; exit 1; }

#
# jobdir  ## Lustre
#
# get_jobdir: setup $jobdir var and makes sure $jobdir is present
# uses: $MOAB_JOBNAME, $PBS_JOBID, $JOBDIRHOME
# sets: $jobdir
#
get_jobdir() {
    if [ x${JOBDIRHOME-} != x ]; then
        jobdirhome=${JOBDIRHOME}
    else
        jobdirhome=${HOME}/jobs
    fi
    if [ x${MOAB_JOBNAME-} != x ]; then
        jobdir=${jobdirhome}/${MOAB_JOBNAME}.${PBS_JOBID-}
    else
        jobdir=${jobdirhome}/`basename $0`.$$  # use top-level script name $0
    fi
    mkdir -p ${jobdir} || die "cannot make jobdir ${jobdir}"
    bbdir=${jobdir} ### XXX: auto set bbdir
    logfile=${jobdir}/$(basename $jobdir).log ### XXX: auto set logfile
    message "-INFO- jobdir = ${jobdir}"
}

#
# bbdir  ## shared XFS on CN only
#
# get_bbdir: setup $bbdir var and makes sure $bbdir is present, otherwise
# falls back to $jobdir if BB is not available
# uses: $DW_JOB_STRIPED, $jobdir
# sets: $bbdir
#
get_bbdir() {
    if [ x${DW_JOB_STRIPED-} = x ]; then
      bbdir=${jobdir}
      message "!!! WARNING !!! missing DW_JOB_STRIPED - putting data in jobdir for this test"
    else
      jobdir_last_component=$(basename $jobdir)
      bbdir="${DW_JOB_STRIPED}/$jobdir_last_component"
      do_mpirun 1 1 "" "" "mkdir -p ${bbdir}" "$logfile" || \
          die "cannot make bbdir ${bbdir}"
      do_mpirun 1 1 "" "" "ls ${bbdir}" "$logfile" || \
          die "cannot ls bbdir ${bbdir}"
      message "-INFO- bbdir = ${bbdir}"
    fi
}

#
# all_nodes
#
# gen_hostfile: generate a list of hosts we have in $jobdir/hosts.txt
# one host per line.
# uses: $PBS_NODEFILE, $jobdir
# sets: $all_nodes
# creates: $jobdir/hosts.txt
#
gen_hostfile() {
    message "-INFO- generating hostfile ${jobdir}/hosts.txt..."

    if [ `which aprun` ]; then
        # Generate hostfile on CRAY and store on disk
        cat $PBS_NODEFILE | uniq | sort > $jobdir/hosts.txt || \
            die "failed to create hosts.txt file"

    else
        # Generate hostfile on Emulab and store on disk
        fqdn_suffix="`hostname | sed 's/^[^\.]*././'`"
        exp_hosts="`/share/testbed/bin/emulab-listall | tr ',' '\n' | \
                    sed 's/$/'$fqdn_suffix'/g'`"

        echo "$exp_hosts" > $jobdir/hosts.txt || \
            die "failed to create hosts.txt file"
    fi

    # Populate a variable with hosts
    all_nodes=$(cat ${jobdir}/hosts.txt)
    num_all_nodes=$(cat ${jobdir}/hosts.txt | sort | wc -l)
    message "-INFO- num hosts = ${num_all_nodes}"
}

#
# generate host list files: $jobdir/vpic.hosts, $jobdir/bbos.hosts
# uses: $PBS_NODEFILE, $jobdir, $nodes, $bbos_buddies
# sets: $vpic_nodes, $bbos_nodes
# creates: $jobdir/vpic.hosts, $jobdir/bbos.hosts
#
gen_hosts() {
    message "-INFO- generating host lists..."

    # XXX: sanity check: # of nodes in list from PBS_NODEFILE or
    # XXX:               emulab-listall >= $nodes+$bbos_buddies

    if [ `which aprun` ]; then
        # Generate host lists on CRAY and store them on disk
        cat $PBS_NODEFILE | uniq | sort | head -n $nodes | \
            tr '\n' ',' | sed '$s/,$//' > $jobdir/vpic.hosts || \
            die "failed to create vpic.hosts file"
        cat $PBS_NODEFILE | uniq | sort | tail -n $bbos_buddies | \
            tr '\n' ',' | sed '$s/,$//' > $jobdir/bbos.hosts || \
            die "failed to create bbos.hosts file"

    else
        # Generate host lists on Emulab and store them on disk
        fqdn_suffix="`hostname | sed 's/^[^\.]*././'`"
        exp_hosts="`/share/testbed/bin/emulab-listall | tr ',' '\n' | \
                    sed 's/$/'$fqdn_suffix'/g'`"

        echo "$exp_hosts" | head -n $nodes | \
            tr '\n' ',' | sed '$s/,$//' > $jobdir/vpic.hosts || \
            die "failed to create vpic.hosts file"
        echo "$exp_hosts" | tail -n $bbos_buddies | \
            tr '\n' ',' | sed '$s/,$//' > $jobdir/bbos.hosts || \
            die "failed to create bbos.hosts file"
    fi

    ### set host list variables ###

    vpic_nodes=$(cat ${jobdir}/vpic.hosts)
    num_vpic_nodes=$(cat ${jobdir}/vpic.hosts | sort | wc -l)
    message "-INFO- num vpic nodes = ${num_vpic_nodes}"

    bbos_nodes=$(cat ${jobdir}/bbos.hosts)
    num_bbos_nodes=$(cat ${jobdir}/bbos.hosts | sort | wc -l)
    message "-INFO- num bbos nodes = ${num_bbos_nodes}"
}

#
# clear_caches: clear node caches on vpic nodes
# uses: $vpic_nodes, $cores, $nodes
#
clear_caches() {
    message "-INFO- clearing node caches..."

    if [ `which aprun` ]; then
        message "!!! WARNING !!! skipping cache clear ... no sudo access on cray"
        #aprun -L $vpic_nodes -n $cores -N $nodes sudo sh -c \
        #    'echo 3 > /proc/sys/vm/drop_caches'
    else
        # this does more than just $vpic_nodes (does them all)
        /share/testbed/bin/emulab-mpirunall sudo sh -c \
            'echo 3 > /proc/sys/vm/drop_caches'
    fi

    message "-INFO- done"
}

#
# do_mpirun: Run CRAY MPICH, ANL MPICH, or OpenMPI run command
#
# Arguments:
# @1 number of processes
# @2 number of processes per node
# @3 array of env vars: ("name1", "val1", "name2", ... )
# @4 host list (comma-separated)
# @5 executable (and any options that don't fit elsewhere)
# @6 outfile1 (optional, typically a job-wise log)
# @7 outfile2 (optional, typically an exp-wise log )
do_mpirun() {
    procs=$1
    ppnode=$2
    if [ ! -z "$3" ]; then
        declare -a envs=("${!3}")
    else
        envs=()
    fi
    hosts="$4"
    exe="$5"
    outfile1="${6:-''}"
    outfile2="${7:-''}"

    envstr=""; npstr=""; hstr=""

    if [ `which aprun` ]; then
        # This is likely a CRAY machine. Deploy an aprun job.

        if [ ${#envs[@]} -gt 0 ]; then
            envstr=`printf -- "-e %s=%s " ${envs[@]}`
        fi

        if [ $ppnode -gt 0 ]; then
            npstr="-N $ppnode"
        fi

        if [ ! -z "$hosts" ]; then
            hstr="-L $hosts"
        fi

        message "[MPIEXEC] aprun $hstr -n $procs $npstr $envstr ${EXTRA_MPIOPTS-} $exe"
        aprun $hstr -n $procs $npstr $envstr ${EXTRA_MPIOPTS-} \
            $exe 2>&1 | tee -a $outfile1 | tee -a $outfile2

    elif [ `which mpirun.mpich` ]; then
        if [ ${#envs[@]} -gt 0 ]; then
            envstr=`printf -- "-env %s %s " ${envs[@]}`
        fi

        if [ $ppnode -gt 0 ]; then
            npstr="-ppn $ppnode"
        fi

        if [ ! -z "$hosts" ]; then
            hstr="--host $hosts"
        fi

        message "[MPIEXEC] mpirun.mpich -np $procs $npstr $hstr $envstr ${EXTRA_MPIOPTS-} $exe"
        mpirun.mpich -np $procs $npstr $hstr $envstr ${EXTRA_MPIOPTS-} \
            $exe 2>&1 | tee -a $outfile1 | tee -a $outfile2

    elif [ `which mpirun.openmpi` ]; then
        if [ ${#envs[@]} -gt 0 ]; then
            envstr=`printf -- "-x %s=%s " ${envs[@]}`
        fi

        if [ $ppnode -gt 0 ]; then
            npstr="-npernode $ppnode"
        fi

        if [ ! -z "$hosts" ]; then
            hstr="--host $hosts"
        fi

        message "[MPIEXEC] mpirun.openmpi -np $procs $npstr $hstr $envstr ${EXTRA_MPIOPTS-} $exe"
        mpirun.openmpi -np $procs $npstr $hstr $envstr ${EXTRA_MPIOPTS-} \
            $exe 2>&1 | tee -a $outfile1 | tee -a $outfile2

    else
        die "could not find a supported mpirun or aprun command"
    fi
}

#
# build_deck: build a vpic deck by copying the deck template to the jobdir,
# adjusting config.h, and then compiling it using the vpic-build.op script.
# the result is placed in $jobdir/current-deck.op (XXX: assume you are only
# going to have one compiled deck at a time).
#
# uses: $dfsu_prefix, $jobdir, $cores
# creates: $jobdir/current-deck.op
#
# Arguments:
# @1 in {"file-per-process", "file-per-particle"}
# @2 particles on x dimension
# @3 particles on y dimension
#
build_deck() {
    px=$2
    py=$3

    ddir=${jobdir}/tmpdeck.$$     # tmp staging area for stacking the deck
    rm -rf ${ddir}
    cp -r ${dfsu_prefix}/decks ${ddir}
    mv ${ddir}/trecon-part/config.h ${ddir}/trecon-part/config.bkp || \
        die "mv failed"

    message "--------------- [INPUT-DECK] --------------"
    message "!!! NOTICE !!! building vpic deck with cores = ${cores}, px = ${px}, py = ${py}"
    message ""

    case $1 in
    "file-per-process")
        cat ${ddir}/trecon-part/config.bkp | \
            sed 's/#define VPIC_DUMPS.*/#define VPIC_DUMPS '$vpic_epochs'/' | \
            sed 's/^#define VPIC_FILE_PER_PARTICLE/\/\/#define VPIC_FILE_PER_PARTICLE/' | \
            sed 's/VPIC_TOPOLOGY_X.*/VPIC_TOPOLOGY_X '$cores'/' | \
            sed 's/VPIC_TOPOLOGY_Y.*/VPIC_TOPOLOGY_Y 1/' | \
            sed 's/VPIC_TOPOLOGY_Z.*/VPIC_TOPOLOGY_Z 1/' | \
            sed 's/VPIC_PARTICLE_X.*/VPIC_PARTICLE_X '$px'/' | \
            sed 's/VPIC_PARTICLE_Y.*/VPIC_PARTICLE_Y '$py'/' | \
            sed 's/VPIC_PARTICLE_Z.*/VPIC_PARTICLE_Z 1/' >  \
                   ${ddir}/trecon-part/config.h || \
            die "config.h editing failed"
        ;;
    "file-per-particle")
        cat ${ddir}/trecon-part/config.bkp | \
            sed 's/#define VPIC_DUMPS.*/#define VPIC_DUMPS '$vpic_epochs'/' | \
            sed 's/^\/\/#define VPIC_FILE_PER_PARTICLE/#define VPIC_FILE_PER_PARTICLE/' | \
            sed 's/VPIC_TOPOLOGY_X.*/VPIC_TOPOLOGY_X '$cores'/' | \
            sed 's/VPIC_TOPOLOGY_Y.*/VPIC_TOPOLOGY_Y 1/' | \
            sed 's/VPIC_TOPOLOGY_Z.*/VPIC_TOPOLOGY_Z 1/' | \
            sed 's/VPIC_PARTICLE_X.*/VPIC_PARTICLE_X '$px'/' | \
            sed 's/VPIC_PARTICLE_Y.*/VPIC_PARTICLE_Y '$py'/' | \
            sed 's/VPIC_PARTICLE_Z.*/VPIC_PARTICLE_Z 1/' >  \
                   ${ddir}/trecon-part/config.h || \
            die "config.h editing failed"
        ;;
    *)
        die "build_deck: VPIC mode not supported"
        ;;
    esac

    # Compile input deck
    (cd ${ddir}/trecon-part && ${dfsu_prefix}/bin/vpic-build.op ./turbulence.cxx \
       2>&1 | tee -a $logfile | tee -a $exp_logfile) || \
           die "compilation failed"

    mv ${ddir}/trecon-part/turbulence.op ${jobdir}/current-deck.op || \
        die "install new current deck failed"

    message "-INFO- ($(ls -lh ${jobdir}/current-deck.op))"
    message ""
    message "-INFO- vpic deck installed at ${jobdir}/current-deck.op"
    message "--------------- [    OK    ] --------------"

    # don't need staging area anymore
    rm -rf ${ddir}
}

#
# do_run: run a vpic experiment
#
# uses: $dfsu_prefix, $jobdir, $bbdir, $cores, $nodes, $logfile, $vpic_nodes,
#        $bb_log_size, $bbos_buddies
#        $jobdir/current-deck.op (precompiled deck)
#        $jobdir/bbos.hosts, $jobdir/vpic.hosts
# creates: an experiment tag: {runtype}_P{particles}_C${cores}_N${nodes}
#          $jobdir/$exp_tag - output directory for the experiment
#          $jobdir/$exp_tag.log - logfile for the output dir
# side effects: changes current directory to $exp_dir
#
# Arguments:
# @1 experiment type in {"baseline", "deltafs", "shuffle_test"}
# @2 number of particles
# @3 ppn XXX: not enabled - some scripts pass a different thing
#
do_run() {
    runtype=$1
    p=$2

    # pp: make a more human readable version of "p"
    if [ $((p / (10**6))) -gt 0 ]; then
        pp="$((p / (10**6)))M"
    elif [ $((p / (10**3))) -gt 0 ]; then
        pp="$((p / (10**3)))K"
    else
        pp="$p"
    fi

    exp_tag="${runtype}_P${pp}_C${cores}_N${nodes}"
    ### log file for this particular exp run ###
    exp_logfile="$jobdir/$exp_tag.log"   ### NOTE !! absolute path !! ###
    cd $jobdir || die "cd to $jobdir failed"
    exp_jobdir="$jobdir/$exp_tag"   ### NOTE !! still on Lustre !! ###
    mkdir -p $exp_jobdir || die "mkdir $exp_jobdir failed"
    cd $exp_jobdir || die "cd to $exp_jobdir failed"

    message "--------------- [  RUNEXP  ] --------------"
    message "!!! NOTICE !!! starting exp ${exp_tag}..."
    message ""

    ### ATTENTION: data might go to bb ###
    exp_dir="$bbdir/$exp_tag"
    do_mpirun 1 1 "" "" "mkdir -p ${exp_dir}" "$logfile" "$exp_logfile" || \
        die "cannot make $exp_dir"

    ### NOTE: cannot cd to $exp_dir since it may land in bb ###

    clear_caches

    ### SPAM ENOUGH TO MAKE BOSS HAPPY ###

    message ""
    message "=================================================================="
    message "!!! Running VPIC ($runtype) with $pp particles on $cores cores !!!"
    message "------------"
    message "> Using ${jobdir}/current-deck.op"
    message "> Job dir is ${jobdir}"
    message "> Experiment dir is ${exp_dir}"
    message "> Log to ${exp_logfile}"
    message "  + Log to ${logfile}"
    message "    + Log to STDOUT"
    message "=================================================================="
    message ""

    case $runtype in
    "baseline")
        ### WRITE PATH ###
        do_mpirun $cores 0 "" "$vpic_nodes" "${jobdir}/current-deck.op" \
            "$logfile" "$exp_logfile"
        if [ $? -ne 0 ]; then
            die "baseline: mpirun failed"
        fi

        do_mpirun 1 1 "" "" "du -sb $exp_dir" "$logfile" "$exp_logfile"
        do_mpirun 1 1 "" "" "du -h $exp_dir/particle" \
            "$logfile" "$exp_logfile"

        ### READ PATH ###
        if [ $do_querying -eq 1 ]; then
            query_particles $runtype $exp_dir $p "$logfile" "$exp_logfile"
        fi
        ;;

    "deltafs")
        if [ $bbos_buddies -gt 0 ]; then
            # Start BBOS servers and clients
            message "BBOS Per-core log size: $((bb_log_size / (2**20)))MB"

            bb_server_list=$(cat $jobdir/bbos.hosts | tr ',' ' ')
            n=1
            for s in $bb_server_list; do
                container_dir=$exp_dir/bbos/containers.$n
                do_mpirun 1 1 "" "" "mkdir -p $container_dir" "$logfile"

                env_vars=("BB_Lustre_chunk_size" "$((2**23))"
                          "BB_Mercury_transfer_size" "$bb_sst_size"
                          "BB_Num_workers" "4"
                          "BB_Server_IP_address" "$s"
                          "BB_Output_dir" "$container_dir"
                          "BB_Max_container_size" "$bb_log_size"
                          "BB_Object_dirty_threshold" "$((2**26))"
                          "BB_Binpacking_threshold" "$bb_log_size")
                do_mpirun 1 1 env_vars[@] "$s" \
                      "${dfsu_prefix}/bin/bbos_server" "$logfile" &

                message "BBOS server started at $s"

                n=$((n + 1))
            done

            sleep 5

            c=1; j=1; p=1
            all_clients=$(echo $vpic_nodes | tr ',' '\n')
            num_clts_per_svr=$((nodes / bbos_buddies))
            for s in $bb_server_list; do
                # Generate string of comma-separated client hostnames for aprun
                filter=$(seq -s, $j $((c * num_clts_per_svr)))
                j=$(((c * num_clts_per_svr) + 1))
                clts=$(echo $all_clients | cut -d ' ' --fields $filter | tr ' ' ',' )
                echo "======== $clts bound to server $c ========="

                # one aprun for set of clients bound to one server
                for (( core = 0; core < cores_per_node; core++ )); do
                    env_vars=("BB_Mercury_transfer_size" "$bb_sst_size"
                            "BB_Object_size" "$bb_log_size"
                            "BB_Server_IP_address" "$s"
                            "BB_Core_num" "$core")

                    do_mpirun $num_clts_per_svr 1 env_vars[@] "$clts" \
		                    "${dfsu_prefix}/bin/bbos_client" "$logfile" &

            	    client_pids[$p]=$! # so we can wait for clients to finish
                    p=$((p+1))
                done

                c=$((c+1))
            done
        fi # $bbos_buddies > 0

        # Start DeltaFS processes
        mkdir -p $exp_dir/metadata || die "deltafs metadata mkdir failed"
        mkdir -p $exp_dir/data || die "deltafs data mkdir failed"
        mkdir -p $exp_dir/plfs || die "deltafs plfs mkdir failed"

        preload_lib_path="${dfsu_prefix}/lib/libdeltafs-preload.so"
        deltafs_srvr_path="${dfsu_prefix}/bin/deltafs-srvr"

        vars=("LD_PRELOAD" "$preload_lib_path"
              "PRELOAD_Deltafs_root" "particle"
              "PRELOAD_Local_root" "${exp_dir}/plfs"
              "PRELOAD_Bypass_deltafs_namespace" "1"
              "PRELOAD_Enable_verbose_error" "1"
              "SHUFFLE_Virtual_factor" "1024"
              "SHUFFLE_Mercury_proto" "bmi+tcp"
              "SHUFFLE_Subnet" "$ip_subnet")

        ### write path ###
        do_mpirun $cores 0 vars[@] "$vpic_nodes" \
                  "$jobdir/current-deck.op" $logfile
        if [ $? -ne 0 ]; then
            die "deltafs: mpirun failed"
        fi

        ### check output size ###
        echo "Output size: `du -sb $exp_dir | cut -f1`" | tee -a $logfile
        if [ -d $exp_dir/plfs/particle ]; then
            du -h $exp_dir/plfs/particle | tee -a $logfile
        fi

        # Waiting for clients to finish data transfer to server
        for c_pid in "${!client_pids[@]}"; do
            wait ${client_pids[$c_pid]}
        done

        if [ $bbos_buddies -gt 0 ]; then
            bb_server_list=$(cat $jobdir/bbos.hosts | tr ' ' ',')

            # Kill BBOS clients and servers
            message ""
            message "Killing BBOS servers"
            do_mpirun $bbos_buddies 0 "" "$bb_server_list" "pkill -SIGINT bbos_server" "$logfile"

            # Wait for BBOS binpacking to complete
            wait
        fi # $bbos_buddies > 0

        ### read path ###
        if [ $do_querying -eq 1 ]; then
            query_particles $runtype $exp_dir $p $logfile
        fi
        ;;

    "shuffle_test")
        np=$3

        # Start DeltaFS processes
        mkdir -p $exp_dir/metadata || die "shuffle test metadata mkdir failed"
        mkdir -p $exp_dir/data || die "shuffle test data mkdir failed"
        mkdir -p $exp_dir/plfs || die "shuffle test plfs mkdir failed"

        preload_lib_path="${dfsu_prefix}/lib/libdeltafs-preload.so"
        deltafs_srvr_path="${dfsu_prefix}/bin/deltafs-srvr"

        vars=("LD_PRELOAD" "$preload_lib_path"
              "PRELOAD_Deltafs_root" "particle"
              "PRELOAD_Local_root" "${exp_dir}/plfs"
              "PRELOAD_Bypass_write" "y"
              "PRELOAD_Enable_verbose_error" "y"
              "SHUFFLE_Virtual_factor" "1024"
              "SHUFFLE_Mercury_proto" "bmi+tcp"
              "SHUFFLE_Subnet" "$ip_subnet")

        do_mpirun $cores $np vars[@] "$vpic_nodes" \
                  "$jobdir/current-deck.op" $logfile
        if [ $? -ne 0 ]; then
            die "deltafs: mpirun failed"
        fi

        echo "Output size: `du -sb $exp_dir | cut -f1`" >> $logfile
        du -sh $exp_dir
        ;;
    esac

    message ""
    message "--------------- [    OK    ] --------------"

    exp_logfile=""
}

#
# query_particles: query particle trajectories
#
# uses: $dfsu_prefix
#
# Arguments:
# @1 experiment type in {"baseline", "deltafs"}
# @2 vpic output directory
# @3 total number of particles
# @4 outfile1 (optional, typically a job-wise log)
# @5 outfile2 (optional, typically an exp-wise log )
query_particles() {
    runtype=$1
    vpicout=$2
    qparts=$3
    outfile1="${4:-''}"
    outfile2="${5:-''}"

    case $runtype in
    "baseline")
        reader_bin="${dfsu_prefix}/bin/vpic-reader"
        nnum=$vpic_epochs
        ;;
    "deltafs")
        reader_bin="${dfsu_prefix}/bin/vpic-deltafs-reader"
        nnum=$cores
        ;;
    *)
        die "query_particles: unknown runtype '$runtype'"
    esac

    if [ $nodes -ge $vpic_epochs ]; then
        ppn=1
    else
        ppn=0
    fi

    message ""
    message "=================================================================="
    message "!!! Query VPIC ($runtype) using $nnum cores !!!"
    message "------------"
    message "> Using ${reader_bin}"
    message "> Experiment dir is ${vpicout}"
    message "> Log to ${outfile1}"
    message "  + Log to ${outfile2}"
    message "    + Log to STDOUT"
    message "=================================================================="
    message ""

    # Query particles to see when the DeltaFS approach breaks compared to old,
    # single-pass approach
    if [ $last -eq 1 ]; then
        do_mpirun $nnum $ppn "" $vpic_nodes "$reader_bin -i $vpicout" \
            "$outfile1" "$outfile2"
    else
        do_mpirun $nnum $ppn "" $vpic_nodes "$reader_bin -i $vpicout -n 1" \
            "$outfile1" "$outfile2"
    fi
}
